{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier x Toxic Content Detection\n",
    "Il presente Notebook mostra l'addestramento ed il testing di un Classificatore basato su Random Forest per il task di Toxic Content Detection.\n",
    "\n",
    "I dati sono stati processati come segue:\n",
    "1. Pulizia del testo (si veda, 'dataset_preprocessing.py')\n",
    "2. Lemmatizzazione con NLTK\n",
    "3. Vettorizzazione con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento del Sistema\n",
    "Il Sistema è ovviamente riaddestrabile a piacere. Si consiglia, tuttavia, dato il tempo necessario per riaddestrare il classificatore, di utilizzare il file pickle 'rf_classifier' per eseguire subito gli esperimenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey what is it talk what is it an exclusive gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bye dont look come or think of comming back to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you are gay or antisemmitian archangel white t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck your filthy mother in the ass dry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30572</th>\n",
       "      <td>chris i dont know who you are talking to but i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30573</th>\n",
       "      <td>operation condor is also named a dirty war can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30574</th>\n",
       "      <td>there is no evidence that this block has anyth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>thanks hey utkarshraj thanks for the kindness ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>from previous revision</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30577 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text  toxic\n",
       "0           cocksucker before you piss around on my work      1\n",
       "1      hey what is it talk what is it an exclusive gr...      1\n",
       "2      bye dont look come or think of comming back to...      1\n",
       "3      you are gay or antisemmitian archangel white t...      1\n",
       "4                 fuck your filthy mother in the ass dry      1\n",
       "...                                                  ...    ...\n",
       "30572  chris i dont know who you are talking to but i...      0\n",
       "30573  operation condor is also named a dirty war can...      0\n",
       "30574  there is no evidence that this block has anyth...      0\n",
       "30575  thanks hey utkarshraj thanks for the kindness ...      0\n",
       "30576                             from previous revision      0\n",
       "\n",
       "[30577 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv(\"./../../datasets/training_set.csv\")\n",
    "training_set_lem = pd.read_csv(\"./../../datasets/training_set_lemmatized.csv\")\n",
    "\n",
    "# Osservazione: il Training Set è stato già ripulito\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia l'addestramento che il testing saranno eseguiti sia sul Dataset \"non-lemmatizzato\" che sul Dataset \"lemmatizzato\". Osserviamo immediatamente che lo spazio delle feature del Dataset \"lemmatizzato\" è inferiore (49188 $<$ 56091) rispetto a quello del Dataset \"non-lemmatizzato\". Ciò ha impatto sia sul tempo necessario per addestrare il classificatore sia sull'accuracy del modello, come verrà mostrato in seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30577, 56091)\n",
      "y_train.shape: (30577,)\n",
      "X_train_lem.shape: (30577, 49188)\n",
      "y_train_lem.shape: (30577,)\n"
     ]
    }
   ],
   "source": [
    "# Vettorizzazione con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(training_set['comment_text'])\n",
    "y_train = training_set['toxic']\n",
    "\n",
    "X_train_lem = vectorizer_lem.fit_transform(training_set_lem['comment_text'])\n",
    "y_train_lem = training_set_lem['toxic']\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "\n",
    "print(\"X_train_lem.shape: \" + str(X_train_lem.shape))\n",
    "print(\"y_train_lem.shape: \" + str(y_train_lem.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "n_estimators = 100\n",
    "model_filename = 'rf_classifier_{}.pkl'.format(n_estimators)\n",
    "model_lem_filename = 'tf_classifier_lem_{}.pkl'.format(n_estimators)\n",
    "cl, cl_lem = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esegui la seguente sottosezione per riaddestrare il Classificatore da capo. Il modello ottenuto verrà persistito nel file 'rf_classifier_{n_estimators}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cl = RandomForestClassifier(n_estimators=n_estimators)\n",
    "cl_lem = RandomForestClassifier(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators: 100\n",
      "Training started on not-Lemmatized Dataset...\n",
      "Training completed! Required time: 0:00:58.505016\n"
     ]
    }
   ],
   "source": [
    "# Addestramento sul Dataset non-lemmatizzato\n",
    "\n",
    "print(\"Estimators: \" + str(n_estimators))\n",
    "print(\"Training started on not-Lemmatized Dataset...\")\n",
    "start = datetime.now()\n",
    "cl.fit(X=X_train, y=y_train)\n",
    "end = datetime.now()\n",
    "print(\"Training completed! Required time: \" + str(end-start))\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(cl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators: 100\n",
      "Training started on Lemmatized Dataset...\n",
      "Training completed! Required time: 0:00:52.821768\n"
     ]
    }
   ],
   "source": [
    "# Addestramento sul Dataset lemmatizzato\n",
    "\n",
    "print(\"Estimators: \" + str(n_estimators))\n",
    "print(\"Training started on Lemmatized Dataset...\")\n",
    "start = datetime.now()\n",
    "cl_lem.fit(X=X_train_lem, y=y_train_lem)\n",
    "end = datetime.now()\n",
    "print(\"Training completed! Required time: \" + str(end-start))\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(cl_lem, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esegui la seguente sottosezione per utilizzare il Classificatore già addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'rb') as f:\n",
    "    cl = pickle.load(f)\n",
    "\n",
    "with open(model_lem_filename, 'rb') as f:\n",
    "    cl_lem = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing del Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "test_set_lem = pd.read_csv(\"./../../datasets/test_set_lemmatized.csv\")\n",
    "\n",
    "test_set.dropna(inplace=True)\n",
    "test_set_lem.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set[test_set['toxic'] != -1]\n",
    "other_set = test_set[test_set['toxic'] == -1]\n",
    "\n",
    "test_set_lem = test_set_lem[test_set_lem['toxic'] != -1]\n",
    "other_set_lem = test_set_lem[test_set_lem['toxic'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (63842, 56091)\n",
      "y_test.shape: (63842,)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test_set['comment_text'])\n",
    "y_test = test_set['toxic']\n",
    "\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n",
    "print(\"y_test.shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_lem.shape: (63842, 49188)\n",
      "y_test_lem.shape: (63842,)\n"
     ]
    }
   ],
   "source": [
    "X_test_lem = vectorizer_lem.transform(test_set_lem['comment_text'])\n",
    "y_test_lem = test_set_lem['toxic']\n",
    "\n",
    "print(\"X_test_lem.shape: \" + str(X_test_lem.shape))\n",
    "print(\"y_test_lem.shape: \" + str(y_test_lem.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predizioni sul Test Set non-Lemmatizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213401835782087\n"
     ]
    }
   ],
   "source": [
    "y_pred = cl.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predizioni sul Test Set Lemmatizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8232511512797218\n"
     ]
    }
   ],
   "source": [
    "y_pred_lem = cl_lem.predict(X_test_lem)\n",
    "print(accuracy_score(y_test_lem, y_pred_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di Supporto\n",
    "Da utilizzare qualora si volesse effettuare una predizione su una frase \"inedita\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulizia della Frase\n",
    "def clean_phrases(phrases):\n",
    "    new_phrases = list()\n",
    "    for phrase in phrases:\n",
    "        # Rimozione di \"\\r\" e \"\\n\"\n",
    "        phrase = re.sub(r'[\\r\\n]+', '', phrase)\n",
    "        # Rimozione di sequenze di \":\" (esempio, \"::::\")\n",
    "        phrase = re.sub(r'::+', '', phrase)\n",
    "        # Rimozione di sequenze di \"=\" (esempio, \"====\")\n",
    "        phrase = re.sub(r'==+', '', phrase)\n",
    "        # Rimozione di sequenze di \"*\" (esempio, \"**\")\n",
    "        phrase = re.sub(r'\\*\\*+', '', phrase)\n",
    "        # Rimozione di sequenze numeriche in formato di indirizzi IP (esempio, \"192.168.1.1\")\n",
    "        phrase = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', '', phrase)\n",
    "        # Rimozione di contenuto compreso tra Parentesi Quadre (esempio, \"[contentContent]\")\n",
    "        phrase = re.sub(r'\\[[^\\[\\]]+\\]', '', phrase)\n",
    "        # Rimozione di Apici, sia singoli che doppi\n",
    "        phrase = re.sub(r\"['\\\"]\", \"\", phrase)\n",
    "\n",
    "        ## La rimozione di particolari caratteri o sequenze di caratteri può portare alla fusione di due token diversi\n",
    "\n",
    "        # Splitting di token in cui compare un segno di interpuzione forte (\"?\", \"!\" e \".\") seguito da una lettera maiuscola\n",
    "        phrase = re.sub(r'([?!\\.])([A-Z]\\w*)', r'\\1 \\2', phrase)\n",
    "        # Splitting di parole fuse (esempio, \"parolaParola\" diventa \"parola Parola\")\n",
    "        phrase = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', phrase)\n",
    "\n",
    "        tokens = word_tokenize(phrase)\n",
    "        lowercase_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        new_phrases.append(' '.join(lowercase_tokens))\n",
    "    \n",
    "    return new_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizzazione della Frase\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_text = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmatized_text.append(lemmatized_token)\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today I went to the office at 9 o'clock and there were a lot of people.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = clean_phrases([text])\n",
    "\n",
    "string = phrases[0]\n",
    "lemmatized_string = lemmatize_text(phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_vec = vectorizer.transform([string])\n",
    "lem_string_to_vec = vectorizer_lem.transform([lemmatized_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(cl.predict(string_to_vec))\n",
    "print(cl_lem.predict(lem_string_to_vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
