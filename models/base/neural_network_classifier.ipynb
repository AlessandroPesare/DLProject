{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network Classifier x Toxic Content Detection\n",
    "Il presente Notebook mostra l'addestramento ed il testing di un Classificatore basato su Neural network per il task di Toxic Content Detection.\n",
    "\n",
    "I dati sono stati processati come segue:\n",
    "1. Pulizia del testo (si veda, 'dataset_preprocessing.py')\n",
    "2. Lemmatizzazione con NLTK\n",
    "3. Vettorizzazione con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/s59pk8px01vb8p_b48z9wxz40000gn/T/ipykernel_1778/2817353859.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Dataset \"non-Lemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"./../../datasets/training_set.csv\")\n",
    "test_data = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "test_data.dropna(inplace=True)\n",
    "test_set = test_data[test_data['toxic']!=-1]\n",
    "# Osservazione: il Training Set è stato già ripulito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (15282,)\n",
      "X_train.shape: (15282, 39767)\n",
      "X_train_lem.shape: (15282, 34238)\n",
      "X_test.shape: (63842, 39767)\n",
      "X_test_lem.shape: (63842, 34238)\n"
     ]
    }
   ],
   "source": [
    "# Vettorizzazione con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "\n",
    "y_train = training_set['toxic']\n",
    "\n",
    "X_train = vectorizer.fit_transform(training_set['comment_text'])\n",
    "\n",
    "X_test = vectorizer.transform(test_set['comment_text'])\n",
    "\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponibile, TensorFlow sta utilizzando la GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 13:32:17.233395: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-10 13:32:17.233426: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#Verifica presenza della GPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verifica il dispositivo attualmente utilizzato da TensorFlow\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('GPU disponibile, TensorFlow sta utilizzando la GPU.')\n",
    "else:\n",
    "    print('GPU non disponibile, TensorFlow sta utilizzando la CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN Model\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Reshape((39767, 1), input_shape=(39767,)),  \n",
    "tf.keras.layers.Conv1D(filters=96, kernel_size=11, strides=4,\n",
    "activation='relu',input_shape=(39767, 1)),\n",
    "tf.keras.layers.MaxPool1D(pool_size=3, strides=2,padding='same'),\n",
    "tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.MaxPool1D(pool_size=3, strides=2,padding='same'),\n",
    "tf.keras.layers.Conv1D(filters=384, kernel_size=3, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Conv1D(filters=256, kernel_size=1, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Dense(155, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(40, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_11 (Reshape)        (None, 39767, 1)          0         \n",
      "                                                                 \n",
      " conv1d_54 (Conv1D)          (None, 9940, 96)          1152      \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPooli  (None, 4970, 96)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_55 (Conv1D)          (None, 4970, 256)         123136    \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPooli  (None, 2485, 256)         0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_56 (Conv1D)          (None, 2485, 384)         295296    \n",
      "                                                                 \n",
      " conv1d_57 (Conv1D)          (None, 2485, 256)         98560     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 2485, 155)         39835     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 2485, 155)         0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2485, 40)          6240      \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 2485, 40)          0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2485, 1)           41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 564260 (2.15 MB)\n",
      "Trainable params: 564260 (2.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "182/478 [==========>...................] - ETA: 2:17 - loss: 0.5231 - accuracy: 0.8063"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing del Sistema, Dataset \"Non Lemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "test_data.dropna(inplace=True)\n",
    "test_set = test_data[test_data['toxic'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_set['toxic']\n",
    "print(\"y_test.shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"./../../datasets/X_test_bert.csv\")\n",
    "print(\"X_test.shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "#Metriche: Accuracy,Precision,Recall\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred_binary)))\n",
    "print(\"Precision: \" + str(precision_score(y_test, y_pred_binary)))\n",
    "print(\"Recall: \" + str(recall_score(y_test, y_pred_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "# Definisci le etichette delle classi\n",
    "classes = ['Classe Negativa', 'Classe Positiva']\n",
    "\n",
    "# Plotta la matrice di confusione\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Dataset \"Lemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_lem = pd.read_csv(\"./../../datasets/training_set_lemmatized.csv\")\n",
    "test_data_lem = pd.read_csv(\"./../../datasets/test_set_lemmatized.csv\")\n",
    "test_set_lem = test_data[test_data['toxic']!=-1]\n",
    "test_data_lem.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vettorizzazione con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "\n",
    "y_train = training_set['toxic']\n",
    "X_train_lem = vectorizer_lem.fit_transform(training_set_lem['comment_text'])\n",
    "X_test_lem = vectorizer_lem.transform(test_set_lem['comment_text'])\n",
    "\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "print(\"X_train_lem.shape: \" + str(X_train_lem.shape))\n",
    "print(\"X_test_lem.shape: \" + str(X_test_lem.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica presenza della GPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verifica il dispositivo attualmente utilizzato da TensorFlow\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('GPU disponibile, TensorFlow sta utilizzando la GPU.')\n",
    "else:\n",
    "    print('GPU non disponibile, TensorFlow sta utilizzando la CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem = X_train_lem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN Model\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Reshape((39767, 1), input_shape=(39767,)),  \n",
    "tf.keras.layers.Conv1D(filters=96, kernel_size=11, strides=4,\n",
    "activation='relu',input_shape=(39767, 1)),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Conv1D(filters=120, kernel_size=5, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Conv1D(filters=240, kernel_size=3, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Conv1D(filters=300, kernel_size=3, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Conv1D(filters=150, kernel_size=1, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.MaxPool1D(pool_size=3, strides=2,padding='same'),\n",
    "tf.keras.layers.Dense(80, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(40, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_lem, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing del Sistema, Dataset \"Non Lemmatizzato\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
