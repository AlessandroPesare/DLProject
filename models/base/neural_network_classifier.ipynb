{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network Classifier x Toxic Content Detection\n",
    "Il presente Notebook mostra l'addestramento ed il testing di un Classificatore basato su Neural network per il task di Toxic Content Detection.\n",
    "\n",
    "I dati sono stati processati come segue:\n",
    "1. Pulizia del testo (si veda, 'dataset_preprocessing.py')\n",
    "2. Lemmatizzazione con NLTK\n",
    "3. Vettorizzazione con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/s59pk8px01vb8p_b48z9wxz40000gn/T/ipykernel_1778/2817353859.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Dataset \"non-Lemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"./../../datasets/training_set.csv\")\n",
    "test_data = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "test_data.dropna(inplace=True)\n",
    "test_set = test_data[test_data['toxic']!=-1]\n",
    "# Osservazione: il Training Set è stato già ripulito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (15282,)\n",
      "X_train.shape: (15282, 39767)\n",
      "X_train_lem.shape: (15282, 34238)\n",
      "X_test.shape: (63842, 39767)\n",
      "X_test_lem.shape: (63842, 34238)\n"
     ]
    }
   ],
   "source": [
    "# Vettorizzazione con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "\n",
    "y_train = training_set['toxic']\n",
    "\n",
    "X_train = vectorizer.fit_transform(training_set['comment_text'])\n",
    "\n",
    "X_test = vectorizer.transform(test_set['comment_text'])\n",
    "\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "\n",
    "print(\"X_train.shape: \" + str(X_train.shape))\n",
    "\n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN Model\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Reshape((39767, 1), input_shape=(39767,)),  \n",
    "tf.keras.layers.Conv1D(filters=33, kernel_size=11, strides=4,\n",
    "activation='relu',input_shape=(39767, 1)),\n",
    "tf.keras.layers.MaxPool1D(pool_size=3, strides=2,padding='same'),\n",
    "tf.keras.layers.Conv1D(filters=125, kernel_size=5, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.MaxPool1D(pool_size=3, strides=2,padding='same'),\n",
    "tf.keras.layers.Conv1D(filters=150, kernel_size=3, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Conv1D(filters=125, kernel_size=3, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Conv1D(filters=125, kernel_size=1, padding='same',\n",
    "activation='relu'),\n",
    "tf.keras.layers.Dense(80, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(40, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 39767, 1)          0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 9940, 45)          540       \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 9940, 45)          0         \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 9940, 125)         28250     \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPoolin  (None, 4970, 125)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 4970, 150)         56400     \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 4970, 125)         56375     \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 4970, 60)          7560      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4970, 80)          4880      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4970, 80)          0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 4970, 40)          3240      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4970, 40)          0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4970, 1)           41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 157286 (614.40 KB)\n",
      "Trainable params: 157286 (614.40 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponibile, TensorFlow sta utilizzando la GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 13:32:17.233395: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-10 13:32:17.233426: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica il dispositivo attualmente utilizzato da TensorFlow\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('GPU disponibile, TensorFlow sta utilizzando la GPU.')\n",
    "else:\n",
    "    print('GPU non disponibile, TensorFlow sta utilizzando la CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "478/478 [==============================] - 164s 328ms/step - loss: 0.5190 - accuracy: 0.7998\n",
      "Epoch 2/10\n",
      "478/478 [==============================] - 256s 537ms/step - loss: 0.5073 - accuracy: 0.7999\n",
      "Epoch 3/10\n",
      "478/478 [==============================] - 129s 269ms/step - loss: 0.5054 - accuracy: 0.7999\n",
      "Epoch 4/10\n",
      "478/478 [==============================] - 128s 269ms/step - loss: 0.5044 - accuracy: 0.7999\n",
      "Epoch 5/10\n",
      "478/478 [==============================] - 128s 268ms/step - loss: 0.5040 - accuracy: 0.7999\n",
      "Epoch 6/10\n",
      "478/478 [==============================] - 128s 269ms/step - loss: 0.5031 - accuracy: 0.7999\n",
      "Epoch 7/10\n",
      "478/478 [==============================] - 130s 272ms/step - loss: 0.5029 - accuracy: 0.7999\n",
      "Epoch 8/10\n",
      "478/478 [==============================] - 130s 272ms/step - loss: 0.5022 - accuracy: 0.7999\n",
      "Epoch 9/10\n",
      "478/478 [==============================] - 132s 275ms/step - loss: 0.5020 - accuracy: 0.7999\n",
      "Epoch 10/10\n",
      "478/478 [==============================] - 132s 275ms/step - loss: 0.5018 - accuracy: 0.7999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a3a9b910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python platform: macOS-14.0-arm64-arm-64bit\n",
      "Tensorflow version: 2.15.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Python platform: {platform.platform()}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Dataset \"Lemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_lem = pd.read_csv(\"./../../datasets/training_set_lemmatized.csv\")\n",
    "test_data_lem = pd.read_csv(\"./../../datasets/test_set_lemmatized.csv\")\n",
    "test_set_lem = test_data[test_data['toxic']!=-1]\n",
    "test_data_lem.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vettorizzazione con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "\n",
    "y_train = training_set['toxic']\n",
    "X_train_lem = vectorizer_lem.fit_transform(training_set_lem['comment_text'])\n",
    "X_test_lem = vectorizer_lem.transform(test_set_lem['comment_text'])\n",
    "\n",
    "print(\"y_train.shape: \" + str(y_train.shape))\n",
    "print(\"X_train_lem.shape: \" + str(X_train_lem.shape))\n",
    "print(\"X_test_lem.shape: \" + str(X_test_lem.shape))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
