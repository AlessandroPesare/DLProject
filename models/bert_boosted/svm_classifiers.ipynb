{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrazione delle features con BERT e confronto delle prestazioni con il classificatore SVM con TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey what is it talk what is it an exclusive gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bye dont look come or think of comming back to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you are gay or antisemmitian archangel white t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck your filthy mother in the ass dry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30572</th>\n",
       "      <td>chris i dont know who you are talking to but i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30573</th>\n",
       "      <td>operation condor is also named a dirty war can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30574</th>\n",
       "      <td>there is no evidence that this block has anyth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>thanks hey utkarshraj thanks for the kindness ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>from previous revision</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30577 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text  toxic\n",
       "0           cocksucker before you piss around on my work      1\n",
       "1      hey what is it talk what is it an exclusive gr...      1\n",
       "2      bye dont look come or think of comming back to...      1\n",
       "3      you are gay or antisemmitian archangel white t...      1\n",
       "4                 fuck your filthy mother in the ass dry      1\n",
       "...                                                  ...    ...\n",
       "30572  chris i dont know who you are talking to but i...      0\n",
       "30573  operation condor is also named a dirty war can...      0\n",
       "30574  there is no evidence that this block has anyth...      0\n",
       "30575  thanks hey utkarshraj thanks for the kindness ...      0\n",
       "30576                             from previous revision      0\n",
       "\n",
       "[30577 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = pd.read_csv(\"./../../datasets/training_set.csv\")\n",
    "# Osservazione: il Training Set è stato già ripulito\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il tokenizer e il modello preaddestrato di BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione di una funzione per estrarre le caratteristiche di un testo utilizzando BERT\n",
    "def extract_features(text):\n",
    "    # return_tensors ritorna il tensore per la versione pytorch\n",
    "    # padding = true fa in modo che frasi di lunghezza diversa vengono portate alla lunghezza massima\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    #rappresentazione media e unidimensionale delle caratteristiche estratte\n",
    "    features = torch.mean(last_hidden_states, dim=1).squeeze()\n",
    "    return features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_data_set(dataset):\n",
    "    features = []\n",
    "    for i in range(len(dataset)):\n",
    "        features.append(extract_features(dataset['comment_text'][i]))\n",
    "    result = pd.DataFrame(features)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_data_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train)\n",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m, in \u001b[0;36mextract_features_data_set\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 4\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(features)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#rappresentazione media e unidimensionale delle caratteristiche estratte\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_Project/lib/python3.11/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = extract_features_data_set(training_set)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_set['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "model_filename = '{}_svm_classifier.pkl'.format(kernel)\n",
    "cl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = SVC(kernel=kernel, probability=True, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training started...\")\n",
    "start = datetime.now()\n",
    "cl.fit(X=X_train, y=y_train)\n",
    "end = datetime.now()\n",
    "print(\"Training completed! Required time: \" + str(end-start))\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(cl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'rb') as f:\n",
    "    cl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing del Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "test_set.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set[test_set['toxic'] != -1]\n",
    "other_set = test_set[test_set['toxic'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cl_lem.predict(X_test)\n",
    "#Metriche: Accuracy,Precision,Recall\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: \" + str(recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brevi esperimenti con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 9915, 2080, 4330, 2072,  999,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "#Esperimento\n",
    "text = \"Ciao Belli!!\"\n",
    "inputs = tokenizer(text,return_tensors=\"pt\", padding=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3141,  0.0321,  0.0595,  ..., -0.1836, -0.0218,  0.6473],\n",
      "         [ 0.2967, -0.0405,  0.5128,  ...,  0.2590,  0.0862,  1.3316],\n",
      "         [-0.3823,  0.0132,  0.9642,  ..., -0.0876, -0.2632,  1.0097],\n",
      "         ...,\n",
      "         [-0.1378, -0.2305,  0.4040,  ...,  0.0642, -0.0660,  0.3863],\n",
      "         [-0.1831, -0.4964, -0.1496,  ...,  0.5253, -0.4985,  0.2009],\n",
      "         [ 0.7862,  0.1154, -0.1137,  ...,  0.2464, -0.8151, -0.0474]]]), pooler_output=tensor([[-0.8261, -0.3466, -0.8242,  0.7842,  0.4768, -0.0715,  0.8891,  0.2735,\n",
      "         -0.7233, -0.9999, -0.4402,  0.8052,  0.9593,  0.5968,  0.9168, -0.7953,\n",
      "         -0.5079, -0.5590,  0.3431, -0.5569,  0.5800,  0.9999,  0.0611,  0.2156,\n",
      "          0.3833,  0.9619, -0.8321,  0.8835,  0.9298,  0.6070, -0.7066,  0.0988,\n",
      "         -0.9773, -0.2711, -0.9180, -0.9770,  0.3035, -0.6644, -0.0661,  0.0302,\n",
      "         -0.8867,  0.1795,  0.9998,  0.3337,  0.2153, -0.4069, -1.0000,  0.2991,\n",
      "         -0.7604,  0.7498,  0.7613,  0.5310,  0.1989,  0.4408,  0.3912, -0.0826,\n",
      "         -0.1643,  0.1459, -0.0984, -0.5176, -0.5825,  0.1556, -0.8790, -0.8488,\n",
      "          0.8427,  0.7372, -0.1757, -0.3278,  0.0561, -0.0669,  0.8545,  0.2512,\n",
      "          0.1183, -0.7961,  0.5351,  0.3084, -0.6444,  1.0000, -0.5171, -0.9497,\n",
      "          0.7223,  0.6130,  0.5338, -0.3194,  0.6159, -1.0000,  0.2869, -0.1696,\n",
      "         -0.9773,  0.2646,  0.4772, -0.0809,  0.5350,  0.5390, -0.6673, -0.3605,\n",
      "         -0.3123, -0.8319, -0.2930, -0.1955,  0.0661, -0.3474, -0.1553, -0.4110,\n",
      "          0.3143, -0.4246, -0.3926,  0.4673,  0.0565,  0.7094,  0.3652, -0.3580,\n",
      "          0.4631, -0.9267,  0.5611, -0.2764, -0.9652, -0.6042, -0.9747,  0.5241,\n",
      "         -0.4012, -0.2063,  0.9106, -0.2249,  0.3170, -0.0148, -0.6419, -1.0000,\n",
      "         -0.5452, -0.6383, -0.2267, -0.2570, -0.9586, -0.8994,  0.4974,  0.9002,\n",
      "          0.1232,  0.9993, -0.2792,  0.8995, -0.4786, -0.7201,  0.6978, -0.3294,\n",
      "          0.7849,  0.2586, -0.6634,  0.1972, -0.1477,  0.1430, -0.5727, -0.2966,\n",
      "         -0.7904, -0.9107, -0.3893,  0.9325, -0.5451, -0.9054, -0.1029, -0.1620,\n",
      "         -0.5012,  0.8381,  0.5809,  0.3816, -0.2728,  0.2879,  0.3116,  0.5115,\n",
      "         -0.8115, -0.0692,  0.3555, -0.3526, -0.8362, -0.9502, -0.4264,  0.5508,\n",
      "          0.9801,  0.7119,  0.2002,  0.5388, -0.0733,  0.6016, -0.9219,  0.9532,\n",
      "         -0.4323,  0.2732, -0.4197,  0.4084, -0.8486,  0.3892,  0.8578, -0.4362,\n",
      "         -0.7177,  0.0250, -0.4886, -0.3719, -0.7254,  0.5216, -0.2143, -0.3087,\n",
      "         -0.0773,  0.8231,  0.9836,  0.7020,  0.1697,  0.6597, -0.8359, -0.4958,\n",
      "          0.0601,  0.3161,  0.2013,  0.9849, -0.6714, -0.0830, -0.8861, -0.9715,\n",
      "          0.0722, -0.8531, -0.0663, -0.7102,  0.5416,  0.1560,  0.5789,  0.3804,\n",
      "         -0.9759, -0.6994,  0.3483, -0.2677,  0.4135, -0.2101,  0.3434,  0.9216,\n",
      "         -0.5099,  0.7235,  0.8957, -0.8681, -0.6791,  0.8437, -0.2098,  0.8241,\n",
      "         -0.5785,  0.9825,  0.8506,  0.6547, -0.8860, -0.6690, -0.8465, -0.7429,\n",
      "         -0.0731,  0.2104,  0.8484,  0.6145,  0.2717,  0.1547, -0.6668,  0.9951,\n",
      "         -0.4164, -0.9163, -0.2510, -0.2925, -0.9737,  0.8323,  0.3372,  0.2773,\n",
      "         -0.4397, -0.5988, -0.9195,  0.8973,  0.1503,  0.9828, -0.0553, -0.9176,\n",
      "         -0.6062, -0.9046, -0.2469, -0.1604, -0.3851, -0.1292, -0.9293,  0.4363,\n",
      "          0.3862,  0.3921, -0.7792,  0.9974,  1.0000,  0.9339,  0.8458,  0.8457,\n",
      "         -0.9991, -0.4887,  1.0000, -0.9626, -1.0000, -0.8695, -0.6707,  0.4401,\n",
      "         -1.0000, -0.1078,  0.0193, -0.8564,  0.5737,  0.9505,  0.9801, -1.0000,\n",
      "          0.6842,  0.8973, -0.6649,  0.9242, -0.4448,  0.9439,  0.8007,  0.1143,\n",
      "         -0.2425,  0.3025, -0.9127, -0.8682, -0.5656, -0.6706,  0.9946,  0.0086,\n",
      "         -0.7560, -0.8931, -0.1375, -0.2023, -0.1064, -0.9214, -0.1126,  0.5412,\n",
      "          0.8056,  0.0504,  0.3638, -0.6326,  0.2486,  0.2298,  0.2403,  0.6537,\n",
      "         -0.9304, -0.5281, -0.4925, -0.1738, -0.7178, -0.9167,  0.9356, -0.4920,\n",
      "          0.7424,  1.0000,  0.4482, -0.7740,  0.5369,  0.2141, -0.5399,  1.0000,\n",
      "          0.7185, -0.9622, -0.5201,  0.5811, -0.4870, -0.5139,  0.9981, -0.1868,\n",
      "         -0.6116, -0.2470,  0.9591, -0.9768,  0.9915, -0.8468, -0.9101,  0.9057,\n",
      "          0.9028, -0.6270, -0.5746,  0.2612, -0.7264,  0.3776, -0.9286,  0.6404,\n",
      "          0.5119,  0.0122,  0.8273, -0.8776, -0.4927,  0.1862, -0.5816, -0.1377,\n",
      "          0.8057,  0.5442, -0.3514, -0.0136, -0.3947, -0.1956, -0.9284,  0.4364,\n",
      "          1.0000, -0.3508,  0.6117, -0.4750,  0.0405, -0.0273,  0.4913,  0.5641,\n",
      "         -0.2205, -0.8682,  0.8000, -0.9401, -0.9706,  0.7925,  0.2626, -0.1924,\n",
      "          1.0000,  0.4703,  0.1336,  0.2989,  0.9615, -0.0045,  0.4867,  0.8580,\n",
      "          0.9608, -0.1792,  0.5094,  0.7407, -0.8352, -0.2708, -0.6196,  0.0191,\n",
      "         -0.8575,  0.0544, -0.9133,  0.9430,  0.8193,  0.3417,  0.2107,  0.6339,\n",
      "          1.0000, -0.3854,  0.7404, -0.5871,  0.8838, -0.9993, -0.7224, -0.3348,\n",
      "         -0.0160, -0.7520, -0.2518,  0.2956, -0.9334,  0.7417,  0.6023, -0.9859,\n",
      "         -0.9772, -0.2965,  0.8727, -0.0186, -0.9477, -0.5895, -0.4001,  0.5729,\n",
      "         -0.3307, -0.8842, -0.1558, -0.2353,  0.4318, -0.1907,  0.5577,  0.8176,\n",
      "          0.5477, -0.4066, -0.0207, -0.0884, -0.8069,  0.8234, -0.7759, -0.8191,\n",
      "         -0.2561,  1.0000, -0.5824,  0.8231,  0.7809,  0.6915, -0.2122,  0.0769,\n",
      "          0.9374,  0.2592, -0.7520, -0.8175, -0.8623, -0.3475,  0.5423,  0.2298,\n",
      "          0.7069,  0.6507,  0.6063,  0.0598,  0.0750, -0.1790,  0.9986, -0.1695,\n",
      "         -0.1545, -0.5240, -0.0665, -0.4304, -0.6851,  1.0000,  0.2416,  0.3832,\n",
      "         -0.9791, -0.7563, -0.9135,  1.0000,  0.7552, -0.6677,  0.6118,  0.6773,\n",
      "         -0.0612,  0.7546, -0.1891, -0.3737,  0.2893,  0.1271,  0.9244, -0.5670,\n",
      "         -0.9503, -0.5728,  0.3750, -0.9351,  0.9997, -0.4992, -0.2984, -0.3909,\n",
      "          0.2640,  0.7724, -0.0882, -0.9699,  0.0261,  0.2211,  0.9319,  0.2023,\n",
      "         -0.5499, -0.8637,  0.6906,  0.6530, -0.8006, -0.9142,  0.9495, -0.9748,\n",
      "          0.5988,  1.0000,  0.3259, -0.3139,  0.0216, -0.3879,  0.1728, -0.2604,\n",
      "          0.6014, -0.8928, -0.3002, -0.0568,  0.3195, -0.1169,  0.3745,  0.6971,\n",
      "          0.1254, -0.4198, -0.4975,  0.0377,  0.3806,  0.8000, -0.2663, -0.1449,\n",
      "         -0.0952, -0.0769, -0.8915, -0.2206, -0.3246, -0.9999,  0.7322, -1.0000,\n",
      "          0.4242,  0.1154, -0.0489,  0.6720, -0.0115,  0.5347, -0.5591, -0.7769,\n",
      "          0.4959,  0.6444, -0.2873, -0.3142, -0.6731,  0.2846, -0.0521,  0.2818,\n",
      "         -0.4816,  0.7770, -0.1105,  1.0000,  0.1714, -0.7265, -0.9590,  0.2040,\n",
      "         -0.1545,  1.0000, -0.8864, -0.9118,  0.3209, -0.5439, -0.8152,  0.2755,\n",
      "         -0.0417, -0.7312, -0.8848,  0.9136,  0.9201, -0.5251,  0.5433, -0.4156,\n",
      "         -0.5849,  0.0870,  0.7350,  0.9682,  0.5049,  0.8088,  0.2482, -0.2797,\n",
      "          0.9357,  0.0764,  0.5924,  0.1678,  1.0000,  0.2677, -0.8568,  0.1710,\n",
      "         -0.9561, -0.1530, -0.9106,  0.1603,  0.1871,  0.8810, -0.2961,  0.9134,\n",
      "         -0.5966,  0.0358, -0.6664, -0.4077,  0.3595, -0.9033, -0.9692, -0.9693,\n",
      "          0.5650, -0.3824, -0.0325,  0.1200,  0.0499,  0.3821,  0.4805, -1.0000,\n",
      "          0.9136,  0.4541,  0.8725,  0.9173,  0.7773,  0.4398,  0.2601, -0.9705,\n",
      "         -0.9520, -0.2711, -0.1195,  0.7760,  0.5920,  0.8280,  0.4444, -0.5086,\n",
      "         -0.2234, -0.4863, -0.3715, -0.9847,  0.4017, -0.4740, -0.9498,  0.9325,\n",
      "          0.0340, -0.2040, -0.1702, -0.6347,  0.9313,  0.7419,  0.3811,  0.1028,\n",
      "          0.5261,  0.7736,  0.9469,  0.9683, -0.7096,  0.7944, -0.4756,  0.4903,\n",
      "          0.6205, -0.8848,  0.1693,  0.2591, -0.4329,  0.1328, -0.1094, -0.9507,\n",
      "          0.6269, -0.2328,  0.6591, -0.4135,  0.0581, -0.4690, -0.0053, -0.6441,\n",
      "         -0.7548,  0.5609,  0.2526,  0.8061,  0.7958, -0.0391, -0.5124, -0.2415,\n",
      "         -0.7109, -0.8553,  0.8923, -0.1005, -0.3720,  0.6491, -0.0709,  0.8408,\n",
      "          0.3005, -0.3810, -0.3472, -0.7082,  0.7241,  0.1008, -0.6151, -0.7620,\n",
      "          0.5707,  0.3200,  0.9999, -0.6967, -0.8199, -0.0184, -0.3624,  0.3582,\n",
      "         -0.4132, -1.0000,  0.4231, -0.3846,  0.7050, -0.7152,  0.5592, -0.6536,\n",
      "         -0.9728, -0.2848,  0.1334,  0.6292, -0.5638, -0.6755,  0.4978, -0.0077,\n",
      "          0.9365,  0.7382, -0.0490,  0.0302,  0.6004, -0.6269, -0.5478,  0.8700]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.17541976e-01 -1.61978051e-01  3.98247629e-01 -1.16937391e-01\n",
      "  3.44952971e-01  1.13185577e-01  8.11842918e-01  6.78107560e-01\n",
      "  1.06444485e-01 -3.62467200e-01 -1.99941128e-01 -4.50574607e-01\n",
      " -1.64689571e-01  5.88219464e-01 -6.61147773e-01  3.77366096e-01\n",
      "  7.35610843e-01 -1.30787075e-01 -1.30586803e-01 -1.42679200e-01\n",
      "  1.95564404e-01 -2.39510741e-02 -2.82505780e-01 -5.05829677e-02\n",
      " -5.75412452e-01 -3.14073026e-01  2.09521845e-01  1.43737588e-02\n",
      "  2.22098544e-01  2.15350702e-01 -2.13940348e-02  1.13563828e-01\n",
      " -3.95994753e-01  5.45265853e-01 -6.50065780e-01  3.46671611e-01\n",
      "  3.95975143e-01  6.89308524e-01 -5.20978570e-01 -5.09267688e-01\n",
      " -5.17328791e-02 -8.04509282e-01 -6.88277334e-02  3.56208384e-01\n",
      "  1.42084792e-01 -4.02675569e-01  2.79372305e-01 -3.74704674e-02\n",
      " -1.98110834e-01 -3.54869038e-01 -1.47073880e-01  7.30344132e-02\n",
      " -5.57606161e-01 -2.65875794e-02 -1.57929689e-01 -2.40625665e-01\n",
      " -2.41708413e-01 -2.64919251e-01 -2.49543525e-02 -2.93899387e-01\n",
      " -3.94180954e-01  1.19030319e-01  7.84161747e-01 -2.96268612e-01\n",
      "  3.27288955e-01 -4.59555211e-03  5.57514392e-02  2.80681282e-01\n",
      " -2.08178669e-01  6.16922341e-02 -1.59704134e-01 -1.67469550e-02\n",
      "  9.91943106e-02 -1.80798426e-01  5.66651113e-02 -2.15070933e-01\n",
      " -3.11035812e-01  4.93050396e-01 -3.84728044e-01 -2.61610478e-01\n",
      " -2.54395269e-02  2.44109645e-01  2.23771378e-01  1.02859616e+00\n",
      " -1.39383689e-01  4.04740632e-01 -3.46550971e-01 -5.68932950e-01\n",
      " -1.05963171e-01  5.96534312e-01 -3.40915024e-01 -9.00495708e-01\n",
      "  2.94646667e-03  1.76435620e-01  5.79534888e-01  3.72438937e-01\n",
      "  9.16115567e-02  2.68420041e-01 -2.25913264e-02 -1.71612337e-01\n",
      " -1.21204309e-01 -3.57240260e-01  3.27301711e-01 -1.98760033e-01\n",
      " -5.57802081e-01  2.85462797e-01  3.13394517e-01 -4.82465655e-01\n",
      " -1.00437649e-01 -6.72998011e-01  3.07479590e-01 -5.39830744e-01\n",
      " -2.41108760e-01 -5.62440455e-01 -5.38034797e-01  4.64969158e-01\n",
      "  3.65658104e-01  5.76012552e-01 -7.80425221e-02 -1.62740469e-01\n",
      "  3.05620998e-01 -1.91918448e-01  4.07457680e-01  6.95602953e-01\n",
      "  2.62193587e-02  4.19714719e-01  2.76320964e-01 -1.51334316e-01\n",
      " -4.77166593e-01 -3.94169122e-01 -2.25926526e-02  8.60507488e-02\n",
      " -6.92877099e-02 -9.89939347e-02 -6.61998272e-01  2.81875432e-01\n",
      " -4.53502424e-02 -3.46848756e-01 -4.80854779e-01  3.70128095e-01\n",
      "  3.08651894e-01  2.29839012e-01 -5.05956113e-01 -1.13067135e-01\n",
      "  1.70572177e-01 -4.27239090e-01 -3.87271821e-01 -8.55451971e-02\n",
      "  7.28406012e-01 -3.86139229e-02  1.52718173e-02 -4.50237215e-01\n",
      "  3.14091265e-01 -2.12104663e-01 -2.24728554e-01 -3.97544242e-02\n",
      " -3.33665699e-01  5.53429067e-01  2.69057006e-01  5.13349950e-01\n",
      "  3.96368444e-01  2.47669727e-01 -5.26895113e-02 -1.82651013e-01\n",
      " -2.17437401e-01 -5.34755476e-02 -2.41322488e-01  6.56590760e-01\n",
      "  3.16782027e-01  4.08320814e-01  4.06697467e-02  2.84165591e-01\n",
      "  7.05882192e-01  4.89955425e-01 -3.39881241e-01  1.91913709e-01\n",
      "  2.75441915e-01  1.35746032e-01 -2.70550907e-01  2.82335013e-01\n",
      " -1.33033645e+00 -2.54677683e-01  1.19671158e-01  1.30373865e-01\n",
      "  4.94309902e-01 -3.39122087e-01  8.49168897e-02 -6.98688507e-01\n",
      " -1.99936748e-01  2.96195745e-02  3.91104728e-01  6.62146360e-02\n",
      " -8.51637870e-02  4.40840535e-02  5.49725555e-02 -2.64865924e-02\n",
      "  2.54478097e-01 -1.50292382e-01  3.55067998e-01 -7.56228045e-02\n",
      "  3.22273895e-02  6.53230026e-02  2.83482671e-01  2.73348749e-01\n",
      " -5.05177557e-01  5.09570003e-01  3.41891021e-01  1.98059250e-02\n",
      "  9.74361151e-02 -1.09541847e-03 -5.39140999e-01 -1.04998183e-02\n",
      "  2.83387005e-01 -1.32677093e-01  4.25317958e-02  1.82468027e-01\n",
      " -2.40921631e-01  2.72539079e-01 -1.88598052e-01 -1.85551077e-01\n",
      "  4.73839551e-01 -3.34305286e-01 -5.12236893e-01  2.59799093e-01\n",
      "  1.84304610e-01  8.81616414e-01  4.47690278e-01  1.66229382e-01\n",
      "  2.19920665e-01 -1.27041459e-01  2.19205260e-01 -1.01586118e-01\n",
      "  3.99981081e-01 -1.91412523e-01 -5.18474638e-01  1.02788053e-01\n",
      "  2.34590545e-01 -5.35038233e-01 -2.35650927e-01 -3.43390435e-01\n",
      " -2.41817579e-01  2.52772719e-01  4.69615847e-01  1.87629387e-01\n",
      " -8.00787564e-03 -3.67913574e-01  7.57793903e-01 -1.21203400e-01\n",
      "  1.55861303e-01  8.95990282e-02 -1.47728086e-01 -2.88197845e-01\n",
      "  3.26373100e-01 -5.52132845e-01  1.84627369e-01 -9.94612694e-01\n",
      " -4.70118597e-02  2.99248248e-01  1.18951559e-01 -1.45721778e-01\n",
      "  2.11581245e-01 -2.95235775e-02  8.26405048e-01 -1.71898112e-01\n",
      " -6.67172968e-01 -2.22763270e-01 -4.51750547e-01  3.61909956e-01\n",
      " -2.55412072e-01 -4.63900976e-02 -6.35785609e-02  8.05427209e-02\n",
      "  2.87468079e-02  1.25355557e-01  2.36671437e-02 -3.92020553e-01\n",
      "  6.02755845e-01  1.85202900e-02  7.28595912e-01 -7.09578991e-02\n",
      "  1.16349936e-01  1.09818444e-01 -4.73898172e-01 -6.34370074e-02\n",
      " -2.97356457e-01 -4.04291958e-01  2.78819025e-01 -1.68281317e-01\n",
      "  1.61755815e-01 -7.42590502e-02  3.94751757e-01 -6.83343887e-01\n",
      " -5.08573830e-01  7.93571845e-02  6.49806038e-02 -2.59476960e-01\n",
      "  2.29854912e-01 -9.13739204e-03  4.50905263e-02 -5.54758728e-01\n",
      " -2.09080979e-01 -1.57527313e-01  6.54421151e-01  4.32647020e-01\n",
      " -9.69482288e-02 -2.76885897e-01  4.86747414e-01 -1.04479872e-01\n",
      " -2.03326535e+00 -2.26642694e-02  3.23555738e-01 -3.17188263e-01\n",
      "  3.78184855e-01 -4.14127052e-01  2.43077382e-01  1.83628667e-02\n",
      " -2.46913701e-01  2.84712166e-01 -5.79151809e-02 -6.59011781e-01\n",
      "  3.99932295e-01  1.76634088e-01 -3.24145854e-01 -3.22357863e-01\n",
      "  5.00695825e-01 -2.10835077e-02 -4.34988201e-01  3.47054422e-01\n",
      " -4.98067066e-02 -1.74921423e-01  4.17569816e-01  6.90169707e-02\n",
      "  7.36015022e-01  2.02356607e-01 -4.09382790e-01 -1.20221376e-01\n",
      " -8.36036086e-01  1.99168950e-01 -6.39191449e-01 -6.27826333e-01\n",
      " -3.82573158e-01 -2.92648822e-01  2.87794679e-01 -4.78858173e-01\n",
      "  8.91256481e-02 -4.25892115e-01 -2.24116638e-01 -2.46436089e-01\n",
      "  5.09363353e-01 -5.24466217e-01  1.44913003e-01 -7.71557912e-02\n",
      "  1.68027848e-01 -2.03562707e-01 -7.57302105e-01 -7.45933592e-01\n",
      "  8.37136153e-03  2.94555247e-01 -3.27385426e-01  1.07955439e-02\n",
      " -2.77377754e-01 -2.01501057e-01  1.92426309e-01  1.11978374e-01\n",
      "  5.34307122e-01  4.12693083e-01  6.64837137e-02  3.76674114e-03\n",
      "  1.37310728e-01  1.34281898e-02 -1.76819131e-01 -3.04059893e-01\n",
      " -3.77249062e-01  2.30376631e-01 -7.81465590e-01 -7.91737437e-02\n",
      " -1.02108821e-01  4.95294362e-01  1.35030285e-01  6.98784739e-02\n",
      " -3.83247584e-01 -1.03742540e+00 -5.47481060e-01  5.56836911e-02\n",
      "  4.01270427e-02  8.65303129e-02  3.53873044e-01  3.67787331e-01\n",
      " -8.43617469e-02 -3.90207320e-01  1.28349900e-01 -5.44699490e-01\n",
      " -5.42219818e-01  1.76250488e-01  3.78349721e-02 -2.71509051e-01\n",
      "  1.24294849e-04 -2.48794660e-01  1.02834769e-01  4.31799471e-01\n",
      "  1.76169634e-01 -7.95220137e-02  3.40615362e-01 -2.59848498e-02\n",
      "  2.17616469e-01  4.89962101e-02  4.92966652e-01  9.73970443e-02\n",
      " -3.35661620e-01 -2.84980804e-01  8.62146556e-01  2.07382679e-01\n",
      "  2.30545253e-02  8.85852729e-04 -1.60567954e-01 -8.04985911e-02\n",
      " -4.19174172e-02  3.22287470e-01  2.07648829e-01 -6.12358630e-01\n",
      "  6.14201725e-02 -4.82927114e-01 -3.42910111e-01 -6.91194713e-01\n",
      "  4.35318917e-01  3.20938528e-01 -3.93590927e-01  8.09161142e-02\n",
      "  4.96793479e-01  2.43709639e-01 -1.96613759e-01 -5.26450694e-01\n",
      " -1.04973388e+00  8.45852271e-02 -2.57344425e-01  1.17740989e-01\n",
      " -2.42504492e-01 -2.64600694e-01 -1.57110915e-01 -1.90368325e-01\n",
      " -5.63444421e-02  4.20304714e-03  2.55591184e-01  1.08513667e-03\n",
      " -3.50889474e-01 -1.27871141e-01  1.75187662e-01  7.14438260e-01\n",
      "  1.62063807e-01  7.01312944e-02 -1.00397313e+00  5.26005141e-02\n",
      "  3.01521450e-01  5.39368510e-01 -8.52396637e-02 -3.02015394e-01\n",
      " -1.93288058e-01  2.43436560e-01  1.90299168e-01 -5.42345464e-01\n",
      "  2.18115732e-01 -6.11334503e-01 -1.03301154e-02  1.43188402e-01\n",
      "  6.97721481e-01 -3.99117142e-01  4.54871915e-02 -4.74651068e-01\n",
      "  1.02924772e-01  2.99871653e-01 -4.93396372e-01  4.74255025e-01\n",
      " -2.10828543e-01  9.19087306e-02 -3.28104757e-02 -1.49705142e-01\n",
      " -2.67664850e-01  1.57738365e-02 -3.15110922e-01  5.52772462e-01\n",
      "  6.12833500e-01 -4.68641594e-02 -3.42355072e-01  5.46790242e-01\n",
      "  2.73974705e-02 -4.21072751e-01 -3.12571794e-01  3.41711283e-01\n",
      "  3.71985584e-01 -1.56995460e-01 -1.73781767e-01  1.20145166e-02\n",
      "  2.54776422e-02 -5.40684238e-02  1.35393053e-01 -6.72535121e-01\n",
      "  5.07878549e-02  1.73247650e-01 -6.85161293e-01  7.55185664e-01\n",
      " -5.50879180e-01 -3.20761502e-01 -4.86347049e-01 -1.75437070e-02\n",
      "  1.41795203e-01 -3.96196306e-01  1.30516022e-01  2.61494815e-01\n",
      "  5.00143170e-01 -1.78499147e-01 -4.44689482e-01  2.14714445e-02\n",
      " -2.87686497e-01 -2.48355092e-03  3.93152148e-01  4.19535398e-01\n",
      "  1.28713325e-01  4.66481745e-01 -4.74162474e-02 -7.39641666e-01\n",
      " -1.47224531e-01 -1.16673671e-01  7.57811517e-02 -2.44294211e-01\n",
      "  6.22509122e-02 -8.80299807e-02 -7.43524075e-01 -2.77227461e-02\n",
      " -7.97124803e-01 -3.31384838e-01 -2.83939928e-01  1.47922963e-01\n",
      " -6.70973897e-01  2.02420773e-03 -7.93546438e-02 -1.20262027e-01\n",
      " -5.65194070e-01 -1.84203386e-01  1.32217452e-01  1.09325215e-01\n",
      " -8.07087272e-02 -1.54502168e-01  4.47357595e-01  3.33060890e-01\n",
      " -1.42885178e-01 -2.87435919e-01  4.42866720e-02  5.77594697e-01\n",
      "  1.30097881e-01 -3.16041440e-01 -2.07020625e-01 -4.63714361e-01\n",
      "  3.84391695e-01  5.00608146e-01  1.29038855e-01  4.32098657e-01\n",
      "  8.90640393e-02  1.61903668e-02 -6.48777708e-02  4.05363411e-01\n",
      " -1.61644325e-01 -5.03040612e-01  1.32625625e-01 -7.99622893e-01\n",
      "  1.20343849e-01  5.11658847e-01  1.90566674e-01 -6.22932352e-02\n",
      " -2.09830880e-01  1.50792703e-01 -6.71877488e-02 -1.94248542e-01\n",
      "  4.28553559e-02  5.53746819e-02  2.79638737e-01  2.73750693e-01\n",
      "  1.25324935e-01 -3.68607640e-02  1.51193991e-01 -3.16818893e-01\n",
      "  6.40567066e-03 -2.09208563e-01 -4.04233903e-01  2.50108279e-02\n",
      "  1.13624029e-01 -6.62464201e-01  6.91348970e-01  1.19175829e-01\n",
      "  1.90677062e-01  7.26554334e-01  1.89555272e-01  2.04894885e-01\n",
      " -2.54461132e-02  9.02214050e-02 -5.95811725e-01  2.92446911e-01\n",
      " -2.25382730e-01  1.04881608e+00  3.67752582e-01  2.37792164e-01\n",
      "  1.12516448e-01  2.56032974e-01  7.11607456e-01  3.31960231e-01\n",
      "  5.17450981e-02  1.25275463e-01 -8.78018677e-01  2.46675655e-01\n",
      "  1.12696588e-01 -2.80793369e-01  3.95120114e-01  1.63519964e-01\n",
      "  2.27985933e-01 -2.52010107e-01 -7.28912232e-03 -1.73344761e-01\n",
      "  8.54952820e-03  3.13024282e-01 -6.04467690e-01 -3.67194384e-01\n",
      "  1.82443470e-01  3.32600594e-01 -3.62720639e-01 -9.83936265e-02\n",
      "  2.36071974e-01 -5.17723441e-01  1.44239306e-01 -4.04388495e-02\n",
      "  1.74047142e-01 -1.92207843e-01  3.90252769e-01 -1.45366400e-01\n",
      "  6.57373071e-01  1.75060138e-01 -5.12772441e-01 -4.38155420e-02\n",
      " -2.86567509e-01  4.93186861e-01  2.14388162e-01  4.10721540e-01\n",
      " -2.42077246e-01  3.79429191e-01 -1.18036933e-01 -8.38045955e-01\n",
      "  3.88602644e-01  3.04475930e-02  8.26426372e-02  3.29914510e-01\n",
      "  4.02260154e-01 -2.24118009e-01  2.99633801e-01  1.17041923e-01\n",
      " -3.00893068e-01 -2.43526176e-01 -1.03138022e-01 -2.73266613e-01\n",
      "  5.09762228e-01  1.95452534e-02  3.33735049e-01  6.75733507e-01\n",
      "  2.02484235e-01 -3.30666006e-01  3.73664945e-01 -7.29267299e-01\n",
      "  1.23074040e-01  1.58576652e-01  3.66253525e-01 -5.13291717e-01\n",
      " -2.74720758e-01  4.42654222e-01  4.48044062e-01 -7.74391294e-02\n",
      "  1.46459024e-02  2.21224546e-01 -4.66413438e-01 -1.41261712e-01\n",
      " -6.32578433e-01  8.93971995e-02 -4.21641409e-01  3.05354536e-01\n",
      " -3.08701813e-01  6.99913144e-01  4.63610083e-01 -1.47413015e-01\n",
      " -6.53753579e-01  1.75713062e-01 -2.05045104e-01  1.90365314e-02\n",
      " -1.22650027e-01 -6.80032745e-02 -1.48538426e-01  9.13339406e-02\n",
      "  4.01890934e-01 -5.35761893e-01 -2.03663439e-01 -4.36923265e-01\n",
      "  1.54622510e-01  1.21925406e-01 -8.26702043e-02 -2.47433975e-01\n",
      " -8.12706947e-02 -4.57091480e-01  1.78279668e-01 -8.84693116e-02\n",
      "  3.61904562e-01  5.16202748e-01  2.81476587e-01  4.24798056e-02\n",
      "  1.33191943e+00 -1.96906067e-02 -3.75114143e-01  1.07323527e-01\n",
      "  8.63925070e-02  1.53922752e-01  2.72376925e-01 -2.14292370e-02\n",
      " -3.95952314e-01 -6.17399037e-01 -1.26916558e-01  4.87289727e-01\n",
      " -9.87915099e-01  6.71005905e-01 -2.88693458e-01 -3.68082702e-01\n",
      "  2.12401107e-01  1.04465961e-01 -3.52181584e-01  1.84714332e-01\n",
      " -3.57955158e-01 -4.52950299e-02  1.11844219e-01 -1.20372616e-01\n",
      "  7.32077882e-02  6.89493567e-02  7.86665305e-02  3.67337197e-01\n",
      "  1.87428564e-01 -2.00179238e-02  5.17607741e-02  7.87895381e-01\n",
      "  4.68057692e-01 -5.03442287e-01 -2.68798647e-03  3.27316254e-01\n",
      "  2.63388425e-01 -5.39591014e-01 -1.24392882e-01 -5.85225940e-01\n",
      "  9.24714267e-01 -4.58164029e-02 -2.80702282e-02 -1.94502115e-01\n",
      " -1.47637069e+00 -5.55807889e-01  2.57656835e-02 -6.11015819e-02\n",
      "  4.37569588e-01 -8.03166866e-01 -2.54401296e-01 -3.67669821e-01\n",
      "  2.71843135e-01 -3.61638546e-01 -7.62269646e-02 -1.11196361e-01\n",
      "  1.33438781e-01  2.71791518e-01 -2.30197877e-01  6.60390198e-01]\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "result = extract_features(\"Ciao Belli!\")\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
