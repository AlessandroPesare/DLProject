{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with BERT (MAC)\n",
    "Il presente Notebook mostra come effettuare l'estrazione delle Feature dal Training e dal Test Set mediante BERT. Attenzione! Il seguente codice risulta differenziato rispetto a quello dei classificatori poiché, per ragioni di efficienza, è stato eseguito su una GPU (Nvidia GeForce RTX 3060) mediante l'ausilio di CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/s59pk8px01vb8p_b48z9wxz40000gn/T/ipykernel_22430/987118529.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/alessandropesare/anaconda3/envs/DL_CUDA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch import cuda\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set.shape: (30577, 2)\n",
      "test_set.shape: (63842, 2)\n",
      "other_set.shape: (89004, 2)\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dei Dataset\n",
    "training_set = pd.read_csv(\"./../../datasets/training_set.csv\")\n",
    "test_data = pd.read_csv(\"./../../datasets/test_set.csv\")\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "test_set = test_data[test_data['toxic'] != -1]\n",
    "other_set = test_data[test_data['toxic'] == -1]\n",
    "\n",
    "print(\"training_set.shape:\", training_set.shape)\n",
    "print(\"test_set.shape:\", test_set.shape)\n",
    "print(\"other_set.shape:\", other_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.0-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is :: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CPU= False\n",
    "device = \"cpu\" if CPU else torch.device(\"mps\")\n",
    "print(\"Device is :: {}\".format(device))\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il tokenizer e il modello preaddestrato di BERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(\"mps\")\n",
    "\n",
    "# L'espressione \"to('mps')\" carica BertModel sulla GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_batches(dataset, batch_size):\n",
    "    return [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(strings):\n",
    "    inputs = tokenizer(strings, return_tensors=\"pt\", padding=True, truncation=True).to(\"mps\")\n",
    "    # return_tensors=\"pt\": ritorna tensori PyTorch\n",
    "    # padding=True: frasi di lunghezza inferiore alla massima vengono adattate ad essa mediante del Padding\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Rappresentazione media e unidimensionale delle caratteristiche estratte\n",
    "    features = torch.mean(last_hidden_states, dim=1).squeeze()\n",
    "    to_return = features.cpu().numpy()\n",
    "\n",
    "    del features\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.mps.empty_cache()\n",
    "    return pd.DataFrame(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per estrarre gli embeddings da un batch di testo (più indicata per l'hate detection)\n",
    "def extract_embeddings(texts):\n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    # Passa i token al modello\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Estrai gli embeddings dall'ultimo layer del modello\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Utilizza l'embedding corrispondente al token CLS\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n",
    "data = training_set['comment_text'][0:32]\n",
    "extraction = extract_features(data.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction for the Training Set...\n",
      "Batch to process: 956\n",
      "Extraction completed! Required Time: 0:30:23.307517\n",
      "            0         1         2         3         4         5         6    \\\n",
      "0      0.305078  0.355401  0.006583  0.119754  0.148529 -0.030381  0.081814   \n",
      "1      0.037936  0.068326  0.065783  0.009710 -0.157437 -0.189122  0.209004   \n",
      "2     -0.054468  0.139920  0.169760  0.011537  0.126337 -0.026550  0.203760   \n",
      "3      0.220874  0.388016  0.100364 -0.068667 -0.147032 -0.154123  0.287599   \n",
      "4      0.263740  0.457342 -0.061892  0.166867  0.133237  0.166459  0.158003   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "30572  0.135423  0.246227  0.125089  0.121697  0.029626 -0.296775  0.370953   \n",
      "30573  0.001573 -0.069304 -0.004758  0.061233 -0.004451 -0.428282 -0.041185   \n",
      "30574  0.097718  0.065837  0.168668 -0.014908 -0.005668 -0.175033  0.109120   \n",
      "30575  0.098884  0.189233  0.299181 -0.050321 -0.090466 -0.037958  0.289672   \n",
      "30576 -0.352978 -0.131679 -0.139642 -0.170231 -0.075853 -0.073617 -0.236303   \n",
      "\n",
      "            7         8         9    ...       758       759       760  \\\n",
      "0      0.060065 -0.052827 -0.037240  ...  0.071560 -0.026473 -0.151490   \n",
      "1      0.177515  0.157365 -0.049735  ... -0.129999 -0.063458  0.066882   \n",
      "2      0.184818 -0.003168 -0.211543  ...  0.106978 -0.166596 -0.142916   \n",
      "3      0.440413  0.066568 -0.164877  ... -0.054784 -0.015228  0.076943   \n",
      "4      0.039106 -0.054911 -0.195428  ...  0.022746 -0.084901 -0.127741   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "30572  0.428036  0.118145 -0.169104  ... -0.014209 -0.195248 -0.078390   \n",
      "30573  0.494684 -0.111796 -0.065351  ...  0.112360  0.262716 -0.075441   \n",
      "30574  0.302468 -0.028603  0.002709  ... -0.020133  0.020293 -0.038787   \n",
      "30575  0.284981 -0.081631 -0.228425  ... -0.087247 -0.121802  0.076404   \n",
      "30576 -0.088878 -0.132768 -0.230164  ...  0.591922  0.177658 -0.238646   \n",
      "\n",
      "            761       762       763       764       765       766       767  \n",
      "0     -0.170573  0.034100  0.034527  0.075658 -0.010538 -0.003976 -0.082579  \n",
      "1     -0.099851 -0.066750  0.054467  0.009865  0.041568  0.018980 -0.099157  \n",
      "2     -0.038621  0.085230 -0.002647 -0.103119  0.006022 -0.019642 -0.009151  \n",
      "3     -0.183917  0.150071  0.047358 -0.083843  0.033975  0.127450 -0.065551  \n",
      "4     -0.229919  0.064310 -0.102914 -0.096813  0.088358  0.096930  0.111939  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "30572 -0.201521 -0.017611  0.181660  0.039373 -0.061882  0.060850 -0.029984  \n",
      "30573 -0.172469  0.157698  0.041479 -0.030234 -0.094155  0.271847  0.071710  \n",
      "30574 -0.222473 -0.119234  0.031736 -0.017138 -0.018136 -0.002663 -0.013080  \n",
      "30575 -0.031314  0.074807  0.146329 -0.191025  0.088958  0.057258 -0.163204  \n",
      "30576 -0.183702  0.295854 -0.271629  0.195214 -0.039613 -0.105458  0.692293  \n",
      "\n",
      "[30577 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Extraction for the Training Set...\")\n",
    "batches = training_set['comment_text']\n",
    "batches = split_to_batches(batches.to_list(), batch_size=32)\n",
    "print(\"Batch to process:\", len(batches))\n",
    "\n",
    "dataframes = list()\n",
    "\n",
    "start = datetime.now()\n",
    "for batch in batches:\n",
    "    extraction = extract_features(batch)\n",
    "    dataframes.append(extraction)\n",
    "end = datetime.now()\n",
    "\n",
    "X_train = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Extraction completed! Required Time:\", str(end-start))\n",
    "print(X_train)\n",
    "\n",
    "X_train.to_csv(\"./X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction for the Test Set...\n",
      "Batch to process: 320\n",
      "Extraction completed! Required Time: 0:12:55.694280\n",
      "X_test.shape: (63842, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Extraction for the Test Set...\")\n",
    "\n",
    "batches = split_to_batches(test_set['comment_text'].to_list(), batch_size=32)\n",
    "print(\"Batch to process:\", len(batches))\n",
    "\n",
    "dataframes = list()\n",
    "\n",
    "start = datetime.now()\n",
    "for batch in batches:\n",
    "    extraction = extract_features(batch)\n",
    "    dataframes.append(extraction)\n",
    "end = datetime.now()\n",
    "\n",
    "X_test = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Extraction completed! Required Time:\", str(end-start))\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "\n",
    "X_test.to_csv(\"./X_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLProjectCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
