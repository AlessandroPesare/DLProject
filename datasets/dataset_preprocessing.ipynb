{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "Il presente Notebook mostra le operazioni di Data Engineering effettuate sul Training Set RAW (\"jigsaw_train_set.csv\") per ottenere il Training Set utilizzato per addestrate i modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto mostrato in questa sezione spiega il funzionamento della funzione \"clean_data\" in \"dataset_preprocessing.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./jigsaw_train_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning\n",
    "Il Cleaning avviene in due fasi:\n",
    "1. Rimozione di particolari caratteri e/o sequenze di caratteri.\n",
    "2. \"De-fusione\" di token accorpati per effetto del primo step di Cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n Do iPods come with AM or FM radios? \\r\\n\\r\\nSince the article does not say, I could assume the answer is \"\"no\"\" but you know what they say about the word assume.  It might be worthwhile to say, somewhere, that other Music Devices also include radios but iPods do not.  (Or do..whichever the case might be.)  -   \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" Do iPods come with AM or FM radios? Since the article does not say, I could assume the answer is \"\"no\"\" but you know what they say about the word assume.  It might be worthwhile to say, somewhere, that other Music Devices also include radios but iPods do not.  (Or do..whichever the case might be.)  -   \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di \"\\r\" e \"\\n\"\n",
    "phrase = train_data[\"comment_text\"][108140]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'[\\r\\n]+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can\\'t even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\r\\n\\r\\n\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can\\'t even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\r\\n\\r\\n\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \":\" (esempio, \"::::\")\n",
    "phrase = train_data[\"comment_text\"][159566]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'::+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"==Change name of section As Non-DST Time==\\r\\nWe must change the name of that section to \"\"As Standard Time\"\" since it would be more confusing if one would read it. If you don\\'t want to remove it you may do this: As Standard (Non-DST) Time\"\". -  \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Change name of section As Non-DST Time\\r\\nWe must change the name of that section to \"\"As Standard Time\"\" since it would be more confusing if one would read it. If you don\\'t want to remove it you may do this: As Standard (Non-DST) Time\"\". -  \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \"=\" (esempio, \"====\")\n",
    "phrase = train_data[\"comment_text\"][62989]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'==+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n ******* Double Standard Against Bosniaks *********** \\r\\n\\r\\nChrisO doesn\\'t want me to use copy of the original investigative article that was published in 1993 by David Bernstein (Pacific News Service). The reason is because he thinks this is a personal website http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html . What difference does it make? It\\'s still original article published 13 years ago by Pacific News Service with full copyright notice? http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html\\r\\n\\r\\nOn the other hand - he allows use of personal \"\"lists\"\" or \"\"groups\"\", such as \"\"mail-archive\"\" and Serb-run \"\"balkanpeace\"\" from Toronto when reading articles republished from Canada\\'s Globe and Mail, example http://www.mail-archive.com/serbian_way@antic.org/msg00008.html\\r\\n\\r\\nAnyways, balkanpeace.org is Serb-run website in which Bosniaks, Croats and other ethnic groups are portrayed as the worst of the worst, while Serb crimes are excused.\\r\\n\\r\\nOne more thing - if I stop being active here, then you will know that they banned me. And if that happens, it will be clear example of pro-Serb one-sidedness and double-standard that is attempting to plague this very important article http://en.wikipedia.org/wiki/Srebrenica_massacre .\\r\\n\\r\\nI urge ChrisO to protect Srebrenica Massacre article in a same way Israel\\'s article is protected http://en.wikipedia.org/wiki/Israel .\\r\\n\\r\\nAnd remember: Srebrenica massacre article is not about Serbs or pro-Serb lobbyists such as General Lewis Mackenzie. Srebrenica massacre article is about 8,000+ victims of genocide. Let\\'s focus on the victims and honor them.\\r\\n\\r\\nHow would you feel if you lost your children, mother, grandmother, grandfather and all people that you loved and lived for? Ask yourself this question every time you edit Srebrenica massacre article. Search for love and compassion in your heart, you will find it.\\r\\n\\r\\nPeace! Always! Forever!\\r\\n\\r\\n \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n  Double Standard Against Bosniaks  \\r\\n\\r\\nChrisO doesn\\'t want me to use copy of the original investigative article that was published in 1993 by David Bernstein (Pacific News Service). The reason is because he thinks this is a personal website http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html . What difference does it make? It\\'s still original article published 13 years ago by Pacific News Service with full copyright notice? http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html\\r\\n\\r\\nOn the other hand - he allows use of personal \"\"lists\"\" or \"\"groups\"\", such as \"\"mail-archive\"\" and Serb-run \"\"balkanpeace\"\" from Toronto when reading articles republished from Canada\\'s Globe and Mail, example http://www.mail-archive.com/serbian_way@antic.org/msg00008.html\\r\\n\\r\\nAnyways, balkanpeace.org is Serb-run website in which Bosniaks, Croats and other ethnic groups are portrayed as the worst of the worst, while Serb crimes are excused.\\r\\n\\r\\nOne more thing - if I stop being active here, then you will know that they banned me. And if that happens, it will be clear example of pro-Serb one-sidedness and double-standard that is attempting to plague this very important article http://en.wikipedia.org/wiki/Srebrenica_massacre .\\r\\n\\r\\nI urge ChrisO to protect Srebrenica Massacre article in a same way Israel\\'s article is protected http://en.wikipedia.org/wiki/Israel .\\r\\n\\r\\nAnd remember: Srebrenica massacre article is not about Serbs or pro-Serb lobbyists such as General Lewis Mackenzie. Srebrenica massacre article is about 8,000+ victims of genocide. Let\\'s focus on the victims and honor them.\\r\\n\\r\\nHow would you feel if you lost your children, mother, grandmother, grandfather and all people that you loved and lived for? Ask yourself this question every time you edit Srebrenica massacre article. Search for love and compassion in your heart, you will find it.\\r\\n\\r\\nPeace! Always! Forever!\\r\\n\\r\\n \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \"*\" (esempio, \"**\")\n",
    "phrase = train_data[\"comment_text\"][146827]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\*\\*+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze numeriche in formato di indirizzi IP (esempio, \"192.168.1.1\")\n",
    "phrase = train_data[\"comment_text\"][0]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase test [contenuto tra parentesi] fine test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase test  fine test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di contenuto compreso tra Parentesi Quadre (esempio, \"[contentContent]\")\n",
    "phrase = \"Frase test [contenuto tra parentesi] fine test\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\[[^\\[\\]]+\\]', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Frase con doppi apici\", \\'token\\''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase con doppi apici, token'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di Apici, sia singoli che doppi\n",
    "phrase = \"\\\"Frase con doppi apici\\\", 'token'\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r\"['\\\"]\", '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bisogna pulire questa frase?Sì'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bisogna pulire questa frase? Sì'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting di token in cui compare un segno di interpuzione forte (\"?\", \"!\" e \".\") seguito da una lettera maiuscola\n",
    "phrase = \"Bisogna pulire questa frase?Sì\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'([?!\\.])([A-Z]\\w*)', r'\\1 \\2', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'questoToken va splittato'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'questo Token va splittato'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting di parole fuse (esempio, \"parolaParola\" diventa \"parola Parola\")\n",
    "phrase = \"questoToken va splittato\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "- Se all'interno di un token ci sono lettere maiuscole, queste vengono rese minuscole\n",
    "- Se un token è costituito da un segno di interpunzione, questo viene rimosso dalla frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto mostrato in questa sezione mostra il funzionamento della funzione \"transform_data\" in \"dataset_preprocessing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Riccardo De\n",
      "[nltk_data]     Cesaris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"CIAO A TUTTI! Questo Notebook mostra come ripulire un Dataset per un task di NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase tokenizzata: ['CIAO', 'A', 'TUTTI', '!', 'Questo', 'Notebook', 'mostra', 'come', 'ripulire', 'un', 'Dataset', 'per', 'un', 'task', 'di', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(phrase)\n",
    "print(\"Frase tokenizzata: \" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase processata: ['ciao', 'a', 'tutti', 'questo', 'notebook', 'mostra', 'come', 'ripulire', 'un', 'dataset', 'per', 'un', 'task', 'di', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "lowercase_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(\"Frase processata: \" + str(lowercase_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CIAO A TUTTI! Questo Notebook mostra come ripulire un Dataset per un task di NLP.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Processata:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ciao a tutti questo notebook mostra come ripulire un dataset per un task di nlp'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_phrase = ' '.join(lowercase_tokens)\n",
    "\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "print(\"Frase Processata:\")\n",
    "display(processed_phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
