{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "Il presente Notebook mostra le operazioni di Data Engineering effettuate sul Training Set RAW (\"jigsaw_train_set.csv\") per ottenere il Training Set utilizzato per addestrate i modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto mostrato in questa sezione spiega il funzionamento della funzione \"clean_data\" in \"dataset_preprocessing.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./jigsaw_train_set.csv\")\n",
    "test_data = pd.read_csv(\"./jigsaw_test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guida al Cleaning delle Stringhe\n",
    "In questa sezione vengono esplicitamente mostrate le soluzioni adottate per il Data Cleaning delle Stringhe e vengono mostrate le trasformazioni eseguite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning\n",
    "Il Cleaning avviene in due fasi:\n",
    "1. Rimozione di particolari caratteri e/o sequenze di caratteri.\n",
    "2. \"De-fusione\" di token accorpati per effetto del primo step di Cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n Do iPods come with AM or FM radios? \\r\\n\\r\\nSince the article does not say, I could assume the answer is \"\"no\"\" but you know what they say about the word assume.  It might be worthwhile to say, somewhere, that other Music Devices also include radios but iPods do not.  (Or do..whichever the case might be.)  -   \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" Do iPods come with AM or FM radios? Since the article does not say, I could assume the answer is \"\"no\"\" but you know what they say about the word assume.  It might be worthwhile to say, somewhere, that other Music Devices also include radios but iPods do not.  (Or do..whichever the case might be.)  -   \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di \"\\r\" e \"\\n\"\n",
    "phrase = train_data[\"comment_text\"][108140]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'[\\r\\n]+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can\\'t even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\r\\n\\r\\n\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can\\'t even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\r\\n\\r\\n\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \":\" (esempio, \"::::\")\n",
    "phrase = train_data[\"comment_text\"][159566]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'::+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"==Change name of section As Non-DST Time==\\r\\nWe must change the name of that section to \"\"As Standard Time\"\" since it would be more confusing if one would read it. If you don\\'t want to remove it you may do this: As Standard (Non-DST) Time\"\". -  \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Change name of section As Non-DST Time\\r\\nWe must change the name of that section to \"\"As Standard Time\"\" since it would be more confusing if one would read it. If you don\\'t want to remove it you may do this: As Standard (Non-DST) Time\"\". -  \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \"=\" (esempio, \"====\")\n",
    "phrase = train_data[\"comment_text\"][62989]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'==+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n ******* Double Standard Against Bosniaks *********** \\r\\n\\r\\nChrisO doesn\\'t want me to use copy of the original investigative article that was published in 1993 by David Bernstein (Pacific News Service). The reason is because he thinks this is a personal website http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html . What difference does it make? It\\'s still original article published 13 years ago by Pacific News Service with full copyright notice? http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html\\r\\n\\r\\nOn the other hand - he allows use of personal \"\"lists\"\" or \"\"groups\"\", such as \"\"mail-archive\"\" and Serb-run \"\"balkanpeace\"\" from Toronto when reading articles republished from Canada\\'s Globe and Mail, example http://www.mail-archive.com/serbian_way@antic.org/msg00008.html\\r\\n\\r\\nAnyways, balkanpeace.org is Serb-run website in which Bosniaks, Croats and other ethnic groups are portrayed as the worst of the worst, while Serb crimes are excused.\\r\\n\\r\\nOne more thing - if I stop being active here, then you will know that they banned me. And if that happens, it will be clear example of pro-Serb one-sidedness and double-standard that is attempting to plague this very important article http://en.wikipedia.org/wiki/Srebrenica_massacre .\\r\\n\\r\\nI urge ChrisO to protect Srebrenica Massacre article in a same way Israel\\'s article is protected http://en.wikipedia.org/wiki/Israel .\\r\\n\\r\\nAnd remember: Srebrenica massacre article is not about Serbs or pro-Serb lobbyists such as General Lewis Mackenzie. Srebrenica massacre article is about 8,000+ victims of genocide. Let\\'s focus on the victims and honor them.\\r\\n\\r\\nHow would you feel if you lost your children, mother, grandmother, grandfather and all people that you loved and lived for? Ask yourself this question every time you edit Srebrenica massacre article. Search for love and compassion in your heart, you will find it.\\r\\n\\r\\nPeace! Always! Forever!\\r\\n\\r\\n \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\r\\n\\r\\n  Double Standard Against Bosniaks  \\r\\n\\r\\nChrisO doesn\\'t want me to use copy of the original investigative article that was published in 1993 by David Bernstein (Pacific News Service). The reason is because he thinks this is a personal website http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html . What difference does it make? It\\'s still original article published 13 years ago by Pacific News Service with full copyright notice? http://www.geocities.com/famous_bosniaks/english/general_lewis_mackenzie.html\\r\\n\\r\\nOn the other hand - he allows use of personal \"\"lists\"\" or \"\"groups\"\", such as \"\"mail-archive\"\" and Serb-run \"\"balkanpeace\"\" from Toronto when reading articles republished from Canada\\'s Globe and Mail, example http://www.mail-archive.com/serbian_way@antic.org/msg00008.html\\r\\n\\r\\nAnyways, balkanpeace.org is Serb-run website in which Bosniaks, Croats and other ethnic groups are portrayed as the worst of the worst, while Serb crimes are excused.\\r\\n\\r\\nOne more thing - if I stop being active here, then you will know that they banned me. And if that happens, it will be clear example of pro-Serb one-sidedness and double-standard that is attempting to plague this very important article http://en.wikipedia.org/wiki/Srebrenica_massacre .\\r\\n\\r\\nI urge ChrisO to protect Srebrenica Massacre article in a same way Israel\\'s article is protected http://en.wikipedia.org/wiki/Israel .\\r\\n\\r\\nAnd remember: Srebrenica massacre article is not about Serbs or pro-Serb lobbyists such as General Lewis Mackenzie. Srebrenica massacre article is about 8,000+ victims of genocide. Let\\'s focus on the victims and honor them.\\r\\n\\r\\nHow would you feel if you lost your children, mother, grandmother, grandfather and all people that you loved and lived for? Ask yourself this question every time you edit Srebrenica massacre article. Search for love and compassion in your heart, you will find it.\\r\\n\\r\\nPeace! Always! Forever!\\r\\n\\r\\n \"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze di \"*\" (esempio, \"**\")\n",
    "phrase = train_data[\"comment_text\"][146827]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\*\\*+', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di sequenze numeriche in formato di indirizzi IP (esempio, \"192.168.1.1\")\n",
    "phrase = train_data[\"comment_text\"][0]\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase test [contenuto tra parentesi] fine test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase test  fine test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di contenuto compreso tra Parentesi Quadre (esempio, \"[contentContent]\")\n",
    "phrase = \"Frase test [contenuto tra parentesi] fine test\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'\\[[^\\[\\]]+\\]', '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Frase con doppi apici\", \\'token\\''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Frase con doppi apici, token'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rimozione di Apici, sia singoli che doppi\n",
    "phrase = \"\\\"Frase con doppi apici\\\", 'token'\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r\"['\\\"]\", '', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bisogna pulire questa frase?Sì'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bisogna pulire questa frase? Sì'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting di token in cui compare un segno di interpuzione forte (\"?\", \"!\" e \".\") seguito da una lettera maiuscola\n",
    "phrase = \"Bisogna pulire questa frase?Sì\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'([?!\\.])([A-Z]\\w*)', r'\\1 \\2', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'questoToken va splittato'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Pulita:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'questo Token va splittato'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting di parole fuse (esempio, \"parolaParola\" diventa \"parola Parola\")\n",
    "phrase = \"questoToken va splittato\"\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "cleaned_phrase = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', phrase)\n",
    "print(\"Frase Pulita:\")\n",
    "display(cleaned_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Refining\n",
    "- Se all'interno di un token ci sono lettere maiuscole, queste vengono rese minuscole\n",
    "- Se un token è costituito da un segno di interpunzione, questo viene rimosso dalla frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto mostrato in questa sezione mostra il funzionamento della funzione \"transform_data\" in \"dataset_preprocessing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Riccardo De\n",
      "[nltk_data]     Cesaris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"CIAO A TUTTI! Questo Notebook mostra come ripulire un Dataset per un task di NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase tokenizzata: ['CIAO', 'A', 'TUTTI', '!', 'Questo', 'Notebook', 'mostra', 'come', 'ripulire', 'un', 'Dataset', 'per', 'un', 'task', 'di', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(phrase)\n",
    "print(\"Frase tokenizzata: \" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase processata: ['ciao', 'a', 'tutti', 'questo', 'notebook', 'mostra', 'come', 'ripulire', 'un', 'dataset', 'per', 'un', 'task', 'di', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "lowercase_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(\"Frase processata: \" + str(lowercase_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase RAW:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CIAO A TUTTI! Questo Notebook mostra come ripulire un Dataset per un task di NLP.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase Processata:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ciao a tutti questo notebook mostra come ripulire un dataset per un task di nlp'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_phrase = ' '.join(lowercase_tokens)\n",
    "\n",
    "print(\"Frase RAW:\")\n",
    "display(phrase)\n",
    "\n",
    "print(\"Frase Processata:\")\n",
    "display(processed_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Lemmatization\n",
    "Per poter agevolare la successiva vettorizzazione delle stringhe, si prevede di lemmatizzare i dataset. Il processo di lemmatizzazione consiste, sostanzialmente, nella riduzione di ogni parola alla sua forma canonica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun if POS tag not found\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_text = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmatized_text.append(lemmatized_token)\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The serene beauty of the sunset paint the sky with hue of orange and pink , captivate all who behold it .\n"
     ]
    }
   ],
   "source": [
    "text_to_lemmatize = \"The serene beauty of the sunset painted the sky with hues of orange and pink, captivating all who beheld it.\"\n",
    "\n",
    "lemmatized_text = lemmatize_text(text_to_lemmatize)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applicazione delle Tecniche descritte ai Dataset\n",
    "1. Pulitura mediante le espressioni regolari\n",
    "2. Standardizzazione, mediante l'eliminazione degli upper cases\n",
    "3. Lemmatizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataset):\n",
    "    ## Verranno eseguiti vari step di pulizia per dati testuali\n",
    "\n",
    "    # Rimozione di \"\\r\" e \"\\n\"\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'[\\r\\n]+', '', x))\n",
    "    # Rimozione di sequenze di \":\" (esempio, \"::::\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'::+', '', x))\n",
    "    # Rimozione di sequenze di \"=\" (esempio, \"====\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'==+', '', x))\n",
    "    # Rimozione di sequenze di \"*\" (esempio, \"**\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'\\*\\*+', '', x))\n",
    "    # Rimozione di sequenze numeriche in formato di indirizzi IP (esempio, \"192.168.1.1\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', '', x))\n",
    "    # Rimozione di contenuto compreso tra Parentesi Quadre (esempio, \"[contentContent]\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'\\[[^\\[\\]]+\\]', '', x))\n",
    "    # Rimozione di Apici, sia singoli che doppi\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r\"['\\\"]\", \"\", x))\n",
    "\n",
    "    ## La rimozione di particolari caratteri o sequenze di caratteri può portare alla fusione di due token diversi\n",
    "\n",
    "    # Splitting di token in cui compare un segno di interpuzione forte (\"?\", \"!\" e \".\") seguito da una lettera maiuscola\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'([?!\\.])([A-Z]\\w*)', r'\\1 \\2', x))\n",
    "    # Splitting di parole fuse (esempio, \"parolaParola\" diventa \"parola Parola\")\n",
    "    dataset[\"comment_text\"] = dataset[\"comment_text\"].apply(lambda x: re.sub(r'([a-z])([A-Z])', r'\\1 \\2', x))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset):\n",
    "    ## Trasformazione di tutte le lettere maiuscole in minuscole e rimozione di tutti i segni di interpunzione\n",
    "\n",
    "    phrases = dataset[\"comment_text\"].to_list()\n",
    "    phrases_cleaned = list()\n",
    "\n",
    "    for phrase in phrases:\n",
    "        tokens = word_tokenize(phrase)\n",
    "        lowercase_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        phrases_cleaned.append(' '.join(lowercase_tokens))\n",
    "\n",
    "    dataset[\"comment_text\"] = pd.Series(phrases_cleaned)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_text = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=pos)\n",
    "        lemmatized_text.append(lemmatized_token)\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data(train_data)\n",
    "train_data = transform_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(dataset):\n",
    "    toxic_entries = dataset[dataset['toxic'] == 1]\n",
    "    non_toxic_entries = dataset[dataset['toxic'] == 0]\n",
    "    print(\"Numero di Frasi 'toxic': \" + str(len(toxic_entries)) + \", Numero di Frasi 'non-toxic': \" + str(len(non_toxic_entries)))\n",
    "\n",
    "    non_toxic_downsampled = resample(non_toxic_entries, n_samples=len(toxic_entries), random_state=42)\n",
    "\n",
    "    final_training_set = pd.concat([toxic_entries, non_toxic_downsampled])\n",
    "    final_training_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    idx_to_remove = list()\n",
    "    for i in range(0, len(final_training_set)):\n",
    "        row = final_training_set.iloc[i]\n",
    "        if row['comment_text'] is '':\n",
    "            idx_to_remove.append(i)\n",
    "  \n",
    "    final_training_set = final_training_set.drop(idx_to_remove)\n",
    "    final_training_set = final_training_set[['comment_text', 'toxic']]\n",
    "\n",
    "    return final_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di Frasi 'toxic': 15294, Numero di Frasi 'non-toxic': 144277\n",
      "training_set.shape: (30577, 2)\n"
     ]
    }
   ],
   "source": [
    "training_set = build_training_set(train_data)\n",
    "print(\"training_set.shape: \" + str(training_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_csv(\"./training_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = training_set['comment_text'].to_list()\n",
    "lemmatized_phrases = list()\n",
    "\n",
    "for phrase in phrases:\n",
    "    lemmatized_phrases.append(lemmatize_text(phrase))\n",
    "\n",
    "training_set['comment_text'] = lemmatized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_csv(\"./training_set_lemmatized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[['comment_text', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = clean_data(test_data)\n",
    "test_data = transform_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"./test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Impossibile trovare il file specificato: 'C:\\\\Users\\\\Riccardo De Cesaris/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28300\\3259351449.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlemmatized_phrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatized_phrases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28300\\2752861427.py\u001b[0m in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mlemmatized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             )\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\DeepLearning\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phrases = test_data['comment_text'].to_list()\n",
    "lemmatized_phrases = list()\n",
    "\n",
    "for phrase in phrases:\n",
    "    lemmatized_phrases.append(lemmatize_text(phrase))\n",
    "\n",
    "test_data['comment_text'] = lemmatized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"./test_set_lemmatized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
